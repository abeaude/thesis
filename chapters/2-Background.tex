\documentclass[../main.tex]{subfiles}
\usepackage{silence}
\WarningFilter{glossaries}{No \printglossary or \printglossaries found}

\begin{document}
\ifSubfilesClassLoaded{%
    \graphicspath{{figures/2-Background/}}%
}{
    \graphicspath{{../figures/2-Background/}}%
}
\chapter{Background}\label{chap:background}
\minitocpage

\section{Deep-Learning models}
 In this section, I will introduce the main deep learning architectures used for discriminative tasks: \glsxtrfull{mlp}, \glsxtrfull{cnn}, \glsxtrfull{gnn}.
 The work of this thesis relies on the application of the attention mechanism, therefore I will detail the attention mechanism and its usage in the transformer architecture.
 I will also describe the \glsxtrfull{ae} and its probabilistic variant the \glsxtrfull{vae} as they are commonly used in the literature to study omics data.
 Finally, I will present the \glsxtrfull{gan} architecture which is used in some of the work in this thesis.

 Let \(\symbf{x}\) be the input and \(y \in \left\{1,2, \ldots, C \right\} \) the associated label, where \(C\) is the number of classes.
 The dimension of \(\symbf{x}\) will be specified in each of the architecture described.
 For discriminative tasks the goal is to return a prediction \(\hat{y} \in {\left[0,1\right]}^C\).
 The goal of the different deep learning architecture is to find the best approximation \(g^{*}\) that maps an input \(\symbf{x}\) into a class \(y\): \(y = g^{*}\left(\symbf{x}\right)\).
 A deep learning architecture define a mapping \(y=g\left(\symbf{x}; \theta\right)\) and learns the parameters \(\theta\) that gives the best function approximation.


 \subsection{\Glsfmtfull{mlp}}\label{subsec:mlp}
     \Gls{mlp} is an architecture taking one-dimensional input, \(\symbf{x} \in \symbb{R}^d\).
     The architecture is composed of an input layer, \(L\) hidden layers and one output layer.
     There are a total of \(L+2\) layers, \(L=0\) corresponds to the input layer and the layer \(L+1\) is the output layer.
     The input layer has \(d\) neurons, one for each input features in \(\symbf{x}\).
     The number of neurons in the output layer depends on the type of task:
     \begin{itemize}
         \item for binary classification, \(C=2\), it has only one neuron,
         \item for multiclass classification \(C > 2 \), it has one neuron per class.
     \end{itemize}
     Each one of the hidden layer \(l\) has a defined number neurons \(N_{l}\).
     Each neuron \(j\) in a layer \(l\) is connected to all neurons in the layer \(l-1\).
     Connection between a neuron \(i\) in layer \(l-1\) and neuron \(j\) in layer \(l\) is weighted by \(w^{l}_{ij}\).
     This type of layer is called a \glsxtrfull{fcn} or dense layer.

     The activation of a neuron \(j\) in layer \(l\) is the non-linear activation \(f\) of the weighted-sum of neurons in layer \(l-1\) and some bias \(b_{j}^{l}\)~(\cref{eq:mlp_neuron}).
     \begin{align}
         a_{j}^{l} & = f\left(\sum_{i}^{N_{l-1}}a_{i}^{l-1}w_{ij}^{l} + b_{j}^{l} \right) \\ \label{eq:mlp_neuron}
                   & = f\left(z^{l}\right)
     \end{align}
     The computation of the activation of all neurons in layer \(l\) can be expressed in a matrix form~(\cref{eq:mlp_matrix}).
     \begin{equation}
         \symbf{a}^{l} = f\left( \symbf{a}^{l-1} \cdot W^l + \symbf{b}^{l} \right) \label{eq:mlp_matrix}
     \end{equation}
     \(W^l \in \symbb{R}^{N_{l-1} \times N_{l}}\) is the weight matrix of general element: \(w_{ij}\), and \(\symbf{b}^{l}\) is the bias vector of layer \(l\).
     The non-linear activation \(f\) is usually a \glsxtrfull{relu}, \(f\left(x\right) = \max\left(0, x\right)\).
     The activation of neurons in layers \(l\)~(\cref{eq:mlp_matrix}) defines a mapping \(g^{l}\) that computes the activation of layer \(l\) from the activation of layer \(l-1\): \( \symbf{a}^{l} = g^{l}\left(\symbf{a}^{l-1}; \theta^{l}\right)\), where \(\theta^{l}\) are the parameters of the layer \(l\): \(W^l\) and \( \symbf{b}^{l}\).
     The activation function of the last layer, layer \(L+1\), depends on the type of task:
     \begin{itemize}
         \item for binary classification, the unique output neuron has a sigmoid activation: \(a^{L+1} = \cfrac{1}{1-\exp\left(z^{L+1}\right)}\) and  yield the probability of the positive class,
         \item for multiclass classification, a softmax activation function is used: \(a_{c}^{L} = \cfrac{\exp\left(z_{c}^{L+1}\right)}{\sum_{j}^{C}\exp\left(z_{j}^{L+1}\right)}\), each \(a_{c}^{L}\) gives the probability that the input \(\symbf{x}\) belongs to the class \(c\).
     \end{itemize}
     The mapping \(g\) of the \gls{mlp} can be expressed as the composition of the mapping of the individual layer: \(g = g^{L+1} \circ g^{L} \circ \cdots \circ g^{1}\).

     \Gls{mlp} are called feedforward networks, the information flows from the input \(\symbf{x}\) to the output \(y\) without any feedback connection, where the output of the model would be fed back to the model itself.


     The \gls{mlp} is one of the simplest model but can have many parameters, especially for high-dimensional inputs.
     Indeed, the number of parameters roughly correspond to the sum of the number of parameters in each weight matrix \(W^{l}\).
     The weight matrix \(W^1\) depends on the input dimension \(d\), the larger \(d\) the higher the number of parameters to learn.

 \subsection{\Glsfmtfull{cnn}}
     \Gls{cnn} is an architecture composed of an input layer, \(L\) hidden layers and one output layer.
     This architecture has been initially developed for images, hence it accepts three-dimensional inputs \(\symbf{x} \in \symbb{R}^{h\times w \times p} \), where \(h\), \(w\) are respectively the height and the width of the image.
     \(p\) represents the numbers of channels, a gray image has only one channel whereas an RGB image has three channels.
     Compared to the \gls{mlp} some hidden layers are replaced with convolutional layers and pooling layers.
     Convolution layers are based on convolution of a one channel input with a two-dimensional kernel \(K\)~(\cref{eq:conv}).
     \begin{equation}
         \left(K * \symbf{x}\right)\left(i,j\right) =\sum_m\sum_n \symbf{x}_{i-m, j-n}K_{m,n} \label{eq:conv}
     \end{equation}
     Usually, multiple filters are applied simultaneously to the multiple input channels.
     In this case the kernel is four-dimensional to account for the in and out channels dimensions~(\cref{eq:conv_multi}).
     \begin{equation}
         \left(K * \symbf{x}\right)\left(i,j, c_{out}\right) =\sum_m\sum_n \symbf{x}_{i-m, j-n, c_{in}}K_{m,n, c_{in}, c_{out}} \label{eq:conv_multi}
     \end{equation}
     After convolving the inputs, a bias term is added and a \gls{relu} activation is applied.
     After one or more convolution layers, a pooling layers is used to introduce some invariance by combining the neurons in fixed window to a single neuron in the next layer.
     The neurons in the window are usually reduced with the maximum or average function.
     The output layer is similar to the one used with the \gls{mlp}, for binary classification a single neuron with a sigmoid activation is used and for multiclass classification problems, there is one neuron per class with a softmax activation~(\cref{subsec:mlp}).

     Similarly to \glspl{mlp}, \glspl{cnn} are feedforward networks, the mapping \(g\) of the \gls{cnn} is also expressed as the composition of the mapping of the individual layers.

     As the \gls{cnn} is constructed around the convolution operator, it can easily be adapted to use one-dimensional input, \(\symbf{x} \in \symbb{R}^d\), by using one-dimensional kernel instead of two-dimensional kernel.

     It should be noted that the use of convolution implies spatial dependencies between input features.

 \subsection{\Glsfmtfull{gnn}}
     \Gls{gnn} is an architecture that processes graphs.
     A graph \(\symcal{G}\) is a structure that consists of vertices \(\symcal{V}\) and edges \(\symcal{E}\).
     An edge \(\left(u,v\right) \in \symcal{V}^2,\, u\neq v\) connects two vertices \(u\) and \(v\), this link represents a relationship between the two vertices.
     The relationship between two nodes can be directed, the relationship is one-way, or undirected.
     In an homogeneous graph, each vertex represent the same type of entities, whereas in an heterogeneous graph each node can represent different type of entities.
     Different type of relationship between two nodes can exist, this is called a multigraph.



     In this subsection, I will focus on undirected graphs.
     introduce graphs, G(E,V), some example of graphs, roads, in biology PPI
     focus on undirected graph
     each node has information associated with itself.
     graph representaion
     permutatio indiferent

     different tasks are possible: graph level, node level and edge level:
     \begin{itemize}
         \item graph level
         \item node level
         \item edge level
     \end{itemize}

     graph convolution networks

 \subsection{Self-attention and transformers}
     To correctly understand a sentence, one need to relate words the other words of the sentence to augment their meaning.
     In machine learning this is achieved by attention mechanisms.
     Attention mechanisms are a simple weighting of the data, it enhances the data elements by dynamically including information about the input's context.
     There are multiple flavor of attentions, the most well-known one is the scaled-dot product attention.
     The self-attention, based on the scaled-dot product attention, takes a two-dimensional input, \(\symbf{x} \in \symbb{R}^{L\times s}\), where \(L\) is the number of input elements and \(s\) the embedding dimension.
     The input \(\symbf{x}\) is projected by three independent trainable linear projection into the query \(Q \in \symbb{R}^{L \times d}\), the key \(K \in \symbb{R}^{L \times d}\) and the value \(V \in \symbb{R}^{L \times d}\):
     \begin{align}
         Q & = \symbf{x}\cdot W^{Q} \\
         K & = \symbf{x}\cdot W^{K} \\
         V & = \symbf{x}\cdot W^{V}
     \end{align}
     Projection matrix \(W^Q \in \symbb{R}^{s \times d} \) (respectively \(W^K\in \symbb{R}^{s \times d}\) and \(W^V\in \symbb{R}^{s \times d}\)) maps the input, \(\symbf{x}\), from an \(s\)-dimensional space to a \(d\)-dimensional space.
     The attention is then applied on those three matrices, \(Q\), \(K\), and \(V\).
     \begin{align}
         \operatorname{Attention}\left(Q,K,T\right) & = \softmax\left( \frac{Q\cdot K^T}{\sqrt{d}}\right) \cdot V\label{eq:selfattention} \\
                                                    & = A \cdot V \label{eq:attention_small}
     \end{align}
     The attention map, \(A \in \symbb{R}^{L \times L}\), measures the similarity between the keys and the query.
     Each row of the attention matrix contains the importance weights of each input elements to the others.
     The information of other input elements is incorporated by multiplying the matrix value by the attention matrix~(\cref{eq:attention_small}).

     Usually, the self-attention computation is performed in \(h\) different heads to learn different types of interactions, this operation is called \gls{mhsa}.
     In each head, the input is projected into a \(\frac{d}{h}\)-dimensional space before applying the attention.
     Each head has its own independent projection matrices: \(W^Q_h \in \symbb{R}^{s \times \frac{d}{h}}\), \(W^K_h \in \symbb{R}^{s \times \frac{d}{h}}\), \(W^V_h \in \symbb{R}^{s \times \frac{d}{h}}\)
     The output of each head, \(H_i\) is concatenated and projected back to a \(d\)-dimensional space with another weight matrix \(W^O \in \symbb{R}^{d \times d}\).
     \begin{align}
         \MHSA\left(X\right) & = \operatorname{concat}\left(\left[H_1, \ldots, H_h \right] \right)\cdot W^O \\
         H_i                 & = \operatorname{Attention}\left(XW^Q_i, XW^K_i, XW^V_i\right)
     \end{align}
     Let \(Z \in \symbb{R}^{L \times d}\) be the output of the \gls{mhsa} operation.

     The attention mechanism is used in the transformer, an encoder-decoder architecture.
     I will only describe the encoder where the self-attention is used.
     The transformer encoder is a stacking of \(n\) blocks, each block is composed of a self-attention and a feed-forward network.
     The feed-forward network transform each element \({\left(Z_i\right)}_{1 \leq i \leq L}\) independently with two successive linear transformation with an intermediate \(\operatorname{ReLU}\) activation~(\cref{eq:ffn}).

     \begin{equation}
         \operatorname{FFN}\left(x\right) = \operatorname{ReLU}\left(xW_1 + B_1\right)W_2 + B_2 \label{eq:ffn}
     \end{equation}
     Usually the input is first projected in a higher dimension, \(4d\) and projected back to its original dimension \(d\).
     In addition to these two elements, the encoder blocks include a layer normalization, known to speed up the training, and skip-connections to prevent vanishing gradients.

     While the transform and in particular the self-attention was primarily designed to study text sequences, it can be applied to various data types that can be represented in a format compatible with the attention mechanism such as images, videos, time series, \dots.
     In this section we mainly describe the self-attention, where the query, key and value are generated from the same input.
     The query can be obtained from one input, and the key, value from another input.
     In this case, we talk about cross-attention.

     One of the major limitations to the scaled-dot product attention is its quadratic complexity both in time and space.
     This quadratic complexity limits the number of elements \(L\) in the inputs to a few thousands.
     To circumvent this limitation new approaches have been proposed to have more efficient transformers architectures.
     Among those methods, some proposed a sparsification of the attention matrix by limiting the attention window to fixed or learnable patterns.
     Another approach proposed a low-rank approximation of the self-attention matrix.
     It has also been proposed to use kernelisation of the attention matrix to enable a write of the attention computation avoid computing the large \(L \times L\) attention matrix.
     Approximation method of the attention matrix has been proposed.
     Some other methods did not change the attention formulation but proposed new algorithm to compute the attention matrix or optimized cuda kernel efficiently leveraging the GPU memory hierarchy.

 \subsection{\Glsfmtfull{ae} and \Glsfmtfull{vae}}
     \Gls{ae} is a feedforward architecture used to learn a latent representation of the data with an unsupervised training.
     The \glsxtrlong{ae} is composed of an input \(\symbf{x}\), an encoder \(E\), a latent representation \(\symbf{z}\), a decoder \(D\) and an output \(\symbf{\hat{x}}\).
     The encoder learns the latent representation of the input by encoding the input: \(\symbf{z} = E\left(\symbf{x}\right)\).
     Then the decoder reconstruct the input from the latent representation: \(\symbf{\hat{x}} = D\left(\symbf{z}\right)\).
     The auto-encoder principles can be applied to numerous architecture and various input data, the encoder \(E\) and the decoder \(D\) can be an \gls{mlp}, a \gls{cnn} or a \gls{gnn}.

     The mapping of the \gls{ae} is expressed as the composition of the encoder and the decoder: \(g\left(\symbf{x}, \theta\right) = D\left(E\left(\symbf{x}, \theta_E\right), \theta_D\right)\), where \(\theta\) corresponds to the parameters of the encoder \(\theta_E\) and the parameters of the decoder \(\theta_D\).

     There are variations of the \gls{ae} that add different regularization to improve the latent representation.
     A commonly used strategy is the \gls{dae}, where a random noise, usually gaussian, is added to the input, and the model aims to reconstruct the denoised input.
     The \gls{vae} adds regularization on the latent space, by constraining the latent representation to follow a multivariate gaussian distribution, each latent feature is represented by a gaussian distribution of parameters \(\mu\) and \(\sigma\).
     The encoder does not anymore learn the latent representation but learns the parameters \(\mu_{\symbf{x}}\) and \(\sigma_{\symbf{x}}\) of the distribution: \(E\left(\symbf{x}\right) = \left(\mu_{\symbf{x}},\sigma_{\symbf{x}}\right) \).
     The latent representation, \(\symbf{z}\), is then sampled from this distribution and fed to the decoder.

     If the latent representation is later used for classification tasks, a classifier branch can be added to the latent representation to predict the corresponding class: \(\hat{y} = \operatorname{Classifier}\left(z\right)\).
     Adding a classification branch should improve the latent representation with more relevant features for the classification.
     In this case, there are two different tasks:
     \begin{enumerate}[nosep]
         \item encode the input and reconstruct it,
         \item predict the corresponding class from the latent representation.
     \end{enumerate}

 \subsection{\Glsfmtfull{gan}}

 \subsection{Model training}
     Earlier, we said that the goal of the different deep learning architectures was to find the best approximation \(g^{*}\) of the mapping \(g\).
     The mapping \(g^{*}\) is found by solving an optimization problem through empirical risk minimization.
     \begin{equation}
         % J\left(\theta\right) = \symbb{E}_{\left(x,y\right)\sim \hat{p}_{data}}\; \symcal{L}\left(g\left(\symbf{x}; \theta\right), y\right)
         \argmin_{\theta} \symcal{L}\left(g\left(\symbf{x}; \theta\right), y\right) \label{eq:dl_opt}
     \end{equation}
     where \(\symcal{L}\) is the cost function.
     In the supervised learning case, learning a discriminative task, \(y\) is the target output.
     In the self-supervised case~(\gls{ae} and \gls{vae}), \(y\) is the training example itself.
     For a multiclass classification problem, the cross-entropy loss is used as a cost function \(\symcal{L}\).
     \begin{equation}
         \symcal{L}_{CE}\left(\theta\right) = \frac{-1}{N}\sum_{i=1}^{N}\sum_{c=1}^{C} w_c y_{i,c}\log\left(\hat{y}_{i,c}\right)
     \end{equation}
     where \(w_c\) denotes the weight, inversely proportional to the class size, of class \(c\) and \(N\) is the number of training examples.
     \(\hat{y}_{i,c}\) is the probability predicted by the model that the example \(i\) belongs to the class \(c\).
     \(y_{i,c}\) is an indicator of the correct class of the sample \(i\):
     \(
     y_{i,c} = \begin{cases}
         1 \quad \text{if}\; y_i = c \, , \\
         0 \quad \text{else}
     \end{cases}
     \).

     The cost function is adapted to the problem, for the \gls{ae} a reconstruction loss is used, the goal is to minimize the difference between the input \(\symbf{x}\) and the reconstructed input \(\hat{x}\):
     \begin{equation}
         \symcal{L}_{AE}\left(\theta\right) = \frac{1}{N}\sum_{i=1}^{N} {\left\|x_i - \hat{x}_i \right\|}_2^{2}
     \end{equation}
     The \gls{vae} cost function is composed of the reconstruction term of the \gls{ae} loss and a regularization term that forces the latent space to follow a gaussian distribution: the Kullback-Leibler divergence (\(\KL\)):
     \begin{equation}
         \symcal{L}_{VAE}\left(\theta\right) = \symcal{L}_{AE}\left(\theta\right) + \KL\left(\symcal{N}\left(\mu_{x}, \sigma_{x}\right), \symcal{N}\left(0, I\right)\right)
     \end{equation}
     The \(\KL\) divergence measure how a probability distribution is different from another probability distribution.

     \begin{equation}
         \symcal{L}_{GAN}\left(\theta\right)
     \end{equation}

     For survival prediction, there are two families: risk-based approach and discrete time approach.
     Risk-based approach are an extension of the Cox model to deep-learning architecture and use partial log-likelihood loss.
     \begin{equation}
         \symcal{L}_{Cox}\left(\theta\right) = \frac{1}{N_{\delta_i = 1}} \sum_{i:\delta_i=1} \left(\hat{y}_i - \log \sum_{j \in \mathcal{R}(T_i)}\phi_j \right)\; \text{,}
     \end{equation}
     where \(\delta_i\) specifies if the event occurred for patient \(i\), \(T_i\) represents the time associated to the event and \(N_{\delta_i = 1}\) is the number of patients for which the event occurred (\(\delta_i = 1\)).
     \(\phi_i = e^{\hat{y}_i}\) is the predicted risk for patient \(i\).
     \(\symcal{R}(T_i) = \{j: T_j > T_i\}\) is the risk set, the set of patients who are still at risk of death at time \(T_i\).

     There is no closed form to the optimization problem describe in \cref{eq:dl_opt} to find the optimal parameters \(\theta^{*}\).
     The minimization of the loss function is achieved with gradient descent.
     The parameters \(\theta\) are randomly initialized, then the parameters are updated in gradient steps by back-propagating a fraction of the error, \ie the gradient of the loss with respect to the parameters: \(\theta_{n+1} = \theta_{n} - \eta \nabla \symcal{L}_{|\theta_n}\left(\theta_{n}\right)\).
     The hyperparameters \(\eta\) is called the learning rate, it controls the convergence speed to the optimal solution.
     Instead of considering all the training data \(N\), data is split in mini-batches of size \(N_b\), and the gradient descent is done on this mini-batches.
     This is called \gls{sgd}.
     There are algorithmic variation to this gradient descent strategy.
     It has been proposed to add a momentum term, It remembers the update \(\delta_{\theta}\) at each iteration and update the weights as a linear combination of the gradient and the previous update.
     RMSprop ensures that the learning rate is adapted for each model weight by normalizing the gradients with the moving average of the squares of gradients.
     Adam combine the previous algorithm, and keep an exponential moving average estimate of the second order moments of the gradients.
     The processing of all mini-batches correspond to one epoch, the training is repeated for multiple epoch until convergence.

     % Describe GAN training

     Deep learning models are sensitive to overfitting.
     Models are complex with numerous parameters, they might learn how to perfectly solve a task on the training set without generalizing to new data.
     The risk of overfitting is increased when number of training samples is limited.
     Different regularization techniques have been designed to prevent this overfitting.
     Batch normalization normalize the signal \(z^{l}\) by re-centering and re-scaling it before applying the activation function \(f\).
     Dropout randomly ignores some neurons during training.
     The ignored neurons are sampled independently from a Bernoulli distribution.
     Regularization terms can be added to the loss such as L1 regularization \({\|\theta\|}_{1}\) which forces model weights to be sparse or L2 regularization \({\|\theta\|}_{2}\) favoring smaller and evenly distributed weights.
     Early stopping monitors a validation metrics, when it no longer improves the training is stopped to prevent overfitting.
     The best way to have good generalization is to train the model on more data.
     A good way to have more data is to generate fake data add them to the training set.
     For images, simple transformation like translation, rotation can be used to generate fake data.
     However, it is not always possible to generate fake data, for instance for omics data there is no such simple transformation.

     A deep learning model and its training strategy have many hyperparameters that can influence the performances of the model.
     A good strategy to select the best hyperparameters is to conduct a random search and compare the performances on a validation set.
     The hyperparameters leading to the best performances are then selected.

     For classification tasks, the performances were evaluated with the error-rate or the accuracy, the recall, the precision, the specificity, F1-score and the \gls{auroc}.
     For survival prediction tasks, performances were evaluated with the concordance-index.
     It estimates that for a pair of individuals, the predicted risks, \(\phi\),  are concordant with their actual survival times.
     \begin{equation}
         \operatorname{C-Index} = \frac{\sum_{i,j}\symbb{1}_{T_j < T_i}\symbb{1}_{\phi_j > \phi_i}\delta_j}{\sum_{i,j}\symbb{1}_{T_j < T_i}\delta_j}
     \end{equation}
     A \(\operatorname{C-Index} = 0.5\) represents a random prediction and \(\operatorname{C-Index} = 1\) corresponds to a perfect ranking.

\section{Cancer biology}
 %https://www.cell.com/fulltext/S0092-8674(11)00127-9
 %https://en.wikipedia.org/wiki/The_Hallmarks_of_Cancer#Emerging_Hallmarks
 %file:///nhome/siniac/abeaude/T%C3%A9l%C3%A9chargements/nrc2827.pdf
 %https://en.wikipedia.org/wiki/Cancer
 %https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10618731/pdf/1142.pdf
 %https://www.cancer.gov/about-cancer/understanding/what-is-cancer
 %https://www.who.int/en/health-topics/cancer#tab=tab_1
 %file:///nhome/siniac/abeaude/T%C3%A9l%C3%A9chargements/Molecular%20Biology%20of%20Human%20Cancers_%20Second%20Edition%20--%20Wolfgang%20A_%20Schulz%20--%202,%202023%20--%20Springer,%20Springer%20Nature%20Switzerland%20AG%20--%209783031162855%20--%20c4af4ff1d84151f27d34687b2ca398f9%20--%20Anna%E2%80%99s%20Archive.pdf
 This section does not presume to be a complete presentation of cancer biology, but aims to provide the necessary keys to understand cancer origins and the different molecular components involved.
 present normal function of a cell with various components, DNA to protein in a cell. Protein = structure = function.
 How this process is regulated: present central dogma, cell regulation
 cancer = dysregulation of the functionning of a cell.
 origin: multiscale, can happens at diferent place of the process.
 talk about cancer hallmarks

 cancer origin, promotion, growth metastasis = dvt of cancer
 \subsection{Signaling in healthy cells}
 \subsection{Cancer}
     causes mutations, environment


\section{Omics data}
 The omics field correspond to the characterization and the quantification of pools of biological molecules.
 The different omics fields studies an \textit{ome}.
 The \textit{ome} suffix is used to refer to all constituents.
 For instance the genome refers to all the genetic information of an organism, and the genomics correspond to the study of the genome.

 The development of high-throughput technologies and the cost decline associated with those technologies helped foster a wide adoption of those methods.
 This adoption enabled a large availability of omics data and in particular multi-omics data which help revolutionized the field of precision medicine.

 To study omics data there are multiple available platforms with different protocols, multiple references and annotations files, and various tools to perform the final quantification.
 This variability leads to data that are highly heterogeneous.
 Omics data are complex data with many variables, for instance, transcriptomics have around 60K different variables, genomics and methylation data can exceed the hundreds of thousands of variables.

 In the following, I will describe the main omics that are used in precision medicine, and how they are obtained.

 \subsection{Genomics}\label{subsec:genomics}
     The genomics correspond to the study of the genome of an organism, \ie all its genetic information.
     The genetic information corresponds to the DNA sequence, which includes protein-coding genes, non-coding genes and other regulatory elements.
     The DNA sequence is obtained with \gls{ngs} techniques.
     In the following I will only describe the Illumina sequencing, a sequencing by synthesis approach.
     After extracting and purifying the DNA of a sample, it is cut in small fragments of hundreds of base-pairs to form the DNA library.
     Then, adapters are added to the fragments; the adapters contain a barcode to identify the sample, the binding site for the sequencing primer, and the complementary sequence to the oligonucleotides on the flow cell.
     The complementary sequence allows the fragment to attach to the flow cell during the sequencing.
     Each fragment is then amplified into distinct clonal clusters by bridge amplification.
     Sequencing is then performed using a cyclic process that adds fluorescently labeled nucleotides one at a time.
     A camera that captures the emitted fluorescence identifies the incorporated base at each position.
     The cycle is repeated numerous times to obtain completed DNA sequences: reads.
     After the sequencing a set of reads is obtained, those reads are aligned to a reference genome with an aligner.
     A variant calling step is then performed to identify single nucleotide variants.
     The variant calling step will quantify the different \gls{snp} in a DNA sequence, in other words it will show the mutations that occur compared to the reference sequence.

     Quantification of \gls{cnv} is done with microarray approaches or sequencing-based approaches.

 \subsection{Transcriptomics}\label{subsec:transcriptomics}
     Transcriptomics studies all the RNAs transcript of an organism, \ie its transcriptome.
     The RNA is extracted from the sample, messenger RNA are enriched with a poly-A selection process.
     After fragmentation of the RNA molecule, a reverse transcriptase is used to convert the transcript into its \gls{cdna} sequence.
     The \gls{cdna} fragment is then sequenced as described in the \cref{subsec:genomics}.
     The reads are aligned to a reference genome, an annotation file is used to count the number of reads that maps to a region known to be a coding gene.
     The resulting count are expressed in \gls{fpkm}, accounting for the sequencing depth and the length of the gene.

 \subsection{Epigenomics}
     Epigenomics is the study of the epigenome, the epigenetic modification of the genetic material.
     Epigenetic modification correspond to a modification of a cell function, a gene regulation without any modification to the DNA sequence.
     Different mechanism can produce such changes:
     \begin{itemize}
         \item \glsxtrfull{dnam},
         \item histone modification,
         \item miRNA,
         \item \glsxtrfull{lncrna}.
     \end{itemize}
     miRNA and \gls{lncrna} quantification is obtained similarly to mRNAs quantification~(\cref{subsec:transcriptomics}).
     Histone modifications are quantified with a mass-spectrometry-based approach, see \cref{subsec:proteomics}.
     \Gls{dnam} is quantified with bead-chip based methylation assay or bisulfite sequencing.
     The bisulfite treatment converts unmethylated cytosine into uracil.
     Then the DNA sequence is sequenced as described in \cref{subsec:genomics}.
     A bisulfite treatment is also applied to the bead-chip assay.
     There are two types of beads for each targeted CpGs on the plate.
     One of the bead correspond to the methylated site and the other to the unmethylated cytosine.
     After hybridization with the bead, a single base extension is performed with labeled nucleotides.
     The plate is then read to quantify the intensities of the methylated and unmethylated beads.
     For each CpG site a ratio of the two intensities is computed, known as the \(\beta\) value.
     A value of one corresponds to a total methylation of the locus and a value of zero to a total unmethylation of the locus.
     There are three different Illumina BeadChip methylation assay, each with a different coverage of the methylation sites in the human genome.
     The \(\beta\) values represent the methylation ratio at the locus level, the methylation ratio at the gene level is computed as the average of all probes located within 1500 base-pairs of a gene transcription start site.

 \subsection{Proteomics}\label{subsec:proteomics}
     Proteomics is the study of all proteins of an organism, also known as the proteome.
     Proteomics is more difficult to study, the DNA content of an organism is the same in all cells whereas the protein content between two cells can strongly differ.
     This is the results of cell differentiation where different genes are expressed in different cell types.
     Quantification of proteins can be achieved with antibodies-based approach and antibody-free methods.
     One of the main antibody-based method is \gls{rppa} which allows for the simultaneous quantification of multiple proteins from multiple samples.
     Samples are immobilized on a solid support and probed with specific antibodies targeted to the proteins of interest.
     One antibody-free approach is based on mass-spectrometry, it is used to determine the amino acid sequence of a protein.
     Proteins are digested into smaller peptides, their mass-to-charge ratios is then measured with mass-spectrometry.
     The signal intensities reflect the quantity of peptides.

\section{Datasets}
 Large multi-omics dataset are limited, although cost of high-throughput technologies has decreased many difficulties remains in the acquisition of multi-omics data.
 \Gls{tcga} is the main dataset with high-quality patient data, \gls{ccle} is another multi-omics dataset on cell-lines.
 Those dataset presents data that were uniformly processed, ensuring that samples were analyzed in the same way.

 \subsection{\Glsfmtfull{tcga}}
     Among the 33 cancer type available in the \gls{tcga} project, we selected the 19 cancers with more than a hundred patients.
     We collected a total of 8777 samples, 8416 were cancerous samples and 361 normal samples from the GDC Data Portal\footnote{\url{https://portal.gdc.cancer.gov/}} with the \textsf{TCGABiolinks} R package.
     For all selected patients we collected, when available, \glsxtrfull{dnam}, \glsxtrfull{mrna}, \glsxtrfull{mirna}, \glsxtrfull{cnv}, and proteomics.
     Clinical annotations are also available for each patient.
     In \gls{tcga} cancerous samples encompass primary solid tumor, recurrent solid tumor and metastasis.
     The normal samples correspond to solid tissue normal, a portion of tissue is samples at a certain distance of the tumor, or are blood derived.
     Multiple predictive endpoints are available in \gls{tcga}:
     \begin{itemize}[nosep]
         \item type of sample (binary classification),
         \item type of cancer (multiclass classification),
         \item cancer subtype (multiclass classification),
         \item metastasis origin (multiclass classification),
         \item cancer stage (multiclass classification),
         \item primary diagnosis (multiclass classification),
         \item disease type (multiclass classification),
         \item survival (risk prediction or survival time prediction).
     \end{itemize}
     This list is non-exhaustive and is here to show the diversity of available tasks in \gls{tcga}, making it a well studied dataset.
     When using the cancer stage as predictive endpoints, it is important to note that in \gls{tcga} patients with the same stage might not share the same properties.
     Indeed, the cancer staging criteria are different between cancers, and those criteria evolves in time.

     % https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8368987/
     %primary origin of a metastasis % https://www.nature.com/articles/s41467-019-13825-8
     %predict primary or metastasis % https://www.frontiersin.org/articles/10.3389/fmolb.2022.913602/full
     % https://www.nature.com/articles/s41598-023-35842-w


     All samples in \gls{tcga} are processed in an harmonized way\footnote{\url{https://gdc.cancer.gov/about-data/gdc-data-processing}} against common reference files\footnote{\url{https://gdc.cancer.gov/about-data/gdc-data-processing/gdc-reference-files}}.
     According to the \gls{tcga} consortium recommendation we removed FFPE samples as they are not suitable for molecular analysis\footnote{\href{https://web.archive.org/web/20150919082952/https://gdac.broadinstitute.org/runs/sampleReports/latest/FPPP_FFPE_Cases.html}{https://gdac.broadinstitute.org/runs/sampleReports/latest/FPPP\_FFPE\_Cases.html}}.
     We also followed their recommendation to select the best replicate\footnote{\href{https://web.archive.org/web/20150919044554/http://gdac.broadinstitute.org/runs/sampleReports/latest/READ_Replicate_Samples.html}{https://gdac.broadinstitute.org/runs/sampleReports/latest/READ\_Replicate\_Samples.html}}.
     In \gls{tcga}, \gls{dnam} data is analyzed with two different methods: Illumina Human Methylation 450 and Human Methylation 27.
     The methylation 450 platform contains more CpG sites but contains 94\% of the CpG sites detected by the methylation 27 method.
     \Gls{dnam} data was restricted to the probes common to both HumanMethylation27 and HumanMethylation450 platforms.
     Features with more than 10\% of NAs values were removed, remaining NA were imputed using \gls{knn} imputation.
     No feature selection was applied, and data were standardized to a zero mean and unit variance.
     Patients with incorrect survival information were removed.

     For the multi-omics study, we restricted the dataset to patients having all modalities present, which lead us to select 5862 patients of 18 different cancers.
     We also considered coding and \gls{ncrna} as two different modalities, as \gls{ncrna} can affect cancer cell fate through various mechanisms~\cite{Grillone2020} and are involved in the oncogenesis~\cite{Toden2021}.
     In this manuscript, we focused on two tasked: cancer type prediction and risk-based survival prediction.
     70\% of the data was used as a training set, 15\% forms the validation set, and the remaining 15\% forms the test set while preserving the proportion of each cancer.
     The training set is used to train the different models, the validation set to select the best model hyperparameters and the test set is used to assess the model performances.

 \subsection{\Glsfmtfull{ccle}}
     This second dataset, available on the DepMap portal\footnote{\url{https://depmap.org/portal/data_page/?tab=allData}}, is composed of \glsxtrfull{dnam}, \glsxtrfull{mrna}, \glsxtrfull{mirna}, \glsxtrfull{cnv}, metabolomics, and proteomics for 536 cells of 16 different cancers.
     No feature selection was applied, and data were standardized to a zero mean and unit variance.
     70\% of the data was used as a training set, 15\% forms the validation set, and the remaining 15\% forms the test set while preserving the proportion of each cancer.

\end{document}
