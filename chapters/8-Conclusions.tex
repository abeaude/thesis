\documentclass[../main.tex]{subfiles}
\usepackage{silence}
\WarningFilter{glossaries}{No \printglossary or \printglossaries found}

\begin{document}
\ifSubfilesClassLoaded{%
	\graphicspath{{figures/8-Conclusions/}}%
	\setcounter{chapter}{7}%
}{
	\graphicspath{{../figures/8-Conclusions/}}%
}
\chapter{Conclusions and Perspectives}
\minitocpage
\section{Conclusions}
\section{Perspectives}
	This section aims to expand on the work described in this manuscript and discuss broader perspectives related to the precision medicine field.
	\subsection{Removing groups in AttOmics}\label{sec:scalar_attention}
		In \cref{chap:attomics}, we introduced AttOmics an architecture based on the self-attention mechanism.
		To avoid self-attention limitations on high-dimensional vectors, we proposed to group features and apply the attention mechanism to these groups.
		While this approach was shown to be effective~(\cref{chap:attomics}), groups are an architectural constraint; removing them would allow for directly considering feature interactions.
		However, the self-attention operation requires elements with an embedding dimension strictly greater than one, since elements of omics profiles are scalar values with no embedding dimension an intermediate transformation is required.
		The direct idea would be to project scalar values into a higher dimensional space and use the recent self-attention approximation~\cite{xiongNystrOmformerNystr2021,Linformer} or new efficient implementation~\cite{FlashAttention,rabeSelfattentionDoesNot2021,bolyaHydraAttentionEfficient2022a} that allows for scaling up to large inputs\footnote{see~\cite{EfficientTransformers} for a larger overview of the possible methods}.
		Nevertheless, there is no justification for projecting scalar values into a higher space, and more importantly, it is unclear what would represent each dimension of the newly obtained vector.

		The attention matrix is obtained by measuring the similarity between pairs of inputs, for vectors this measure is the scalar product.
		One could consider a similarity measure for scalar values, such as absolute difference, and inject this in the attention computation:
		\begin{equation}
			A_{ij} = \operatorname{softmin}\left(\left|Q_{i} - K_{j} \right| \right) \label{eq:scalar_attention}
		\end{equation}
		where \(Q_{i}\) is the \(i\)-th value in the query vector and \(K_{j}\) is the \(j\)-th value in the key vector\footnote{Note that query, key and value are no longer matrices but vectors.}.
		\pythoncode[label={code:torch_att}]{Pytorch implementation of the ScalarAttention}{figures/8-Conclusions/ScalarAttentionPyTorch.py}
		This scalar formulation of the self-attention~(\cref{eq:scalar_attention}) can be easily implemented in PyTorch~(\cref{code:torch_att}) and has already been tested in~\cite{Lacan2023} but still requires to compute and store large \(d\times d\) attention matrices~(\cref{fig:lin_attention_benchA}).
		\citeauthor{Lacan2023} masked the attention matrix and only computed attention for features pairs known to be interacting (\gls{ppi})\cite{Lacan2023}.
		The \(i\)-th element of the attention output is computed as the scalar product of the \(i\)-th row of the attention matrix with the value vector, and the \(i\)-th row of the attention matrix corresponds to the absolute difference of the query vector with the \(i\)-th value of the key vector.
		\begin{equation}
			Z_{i} = \left| Q - K_{i} \right| \cdot V
		\end{equation}
		This formulation can be implemented in an optimized triton kernel~\cite{TritonLang}, see~\cref{code:triton_att}, with a lower memory footprint withtout significantly increase the computation time~(\cref{fig:lin_attention_bench}).
		\pythoncode[label={code:triton_att}]{Triton implementation of the ScalarAttention}{figures/8-Conclusions/ScalarAttention.py}

		\begin{figure}[htbp]
			\centering
			\begin{subcaptiongroup}
				%\includegraphics{}
				\phantomcaption\label{fig:lin_attention_benchA}
				\phantomcaption\label{fig:lin_attention_benchB}
			\end{subcaptiongroup}
			\caption{Comparison of the memory usage~\subref{fig:lin_attention_benchA} and the computation time~\subref{fig:lin_attention_benchB} of a \textcolor[HTML]{3969AC}{PyTorch} and a \textcolor[HTML]{F2B701}{Triton} implementation of the ScalarAttention}
			\label{fig:lin_attention_bench}
		\end{figure}

		Considering direct features interactions in omics profiles would open up the possibility to regularize the attention map with known interactions.

	\subsection{Extending CrossAttOmics to other type of modalities}
		In \cref{chap:crossattomics}, we introduced CrossAttOmics a deep learning architecture to integrate multi-omics data.
		As the attention mechanism can adapt any type of modalities, such as text or images, with the correct dimensions, a natural extension would be to consider non omics modalities such as histopathology images, clinical information or textual reports.
		The main question would be how to connect them with the rest of the omics interactions graph~(\cref{fig:tcga_graph}).
		Furthermore, the ScalarAttention proposed in \cref{sec:scalar_attention} can be extended to multimodal context, \ie{}a CrossScalarAttention where the query comes from one modality and the key from another one.
		Using such an attention mechanism would refine the considered interactions to the feature level.
		A modality interaction graph would define on which modality pairs the cross-attention is computed and the cross-attention matrix could then be masked or regularized\footnote{see discussion on knowledge integration in \cref{sec:persp_knwoledged}.} to focus on direct relation ships.
		For instance an \gls{mirna} is expected to only interact with its \gls{mrna} targets\footnote{some \gls{mrna} targets are experimentally verified and other are putative targets based on the sequence complementarity.}.

		% TODO: FIGURE to show masked attention map
	\subsection{Role of interpretability in precision medicine}
		consider a model that is able to find a drug that is working in a group of patients
		do we need to know why to treat patient, adverse effect have a model that can accurately predict adverse effects
		but from a research standpoint it is interesting to how the drug works but more importantly why the drug is not working is other patients.

	\subsection{Biological knowledge}\label{sec:persp_knwoledged}
		Biolgically-informed architectures are common when applying deep learning to biological problems as they provide a strong inductive bias.
		However, adding knowledge into the deep learning architectures is challenging as they are things that are still unknown\footnote{This is probably more a philosophical question to know whether we have a complete understanding of the biological mechanism or if there is still some to be discovered.}.
		For instance, annotations mainly cover coding genes, but noncoding genes are known to be involved in gene expression regulation.
		Focusing only on annotated genes would leave out many features\footnote{For instance in \gls{go} only XX \gls{ncrna} are annotated} and limits model capacity to consider all interactions, which at the end might limit the predictive performances.
		Moreover, excluding unannotated features prevents researchers from discovering new biomarkers that could be detected by training models on all available features.

		Another aspect of knowledge integration is the continual construction of our understanding of biological mechanisms; some things we consider true become invalid, and newly discovered things enrich our knowledge.
		This means that a model constructed at time \(T\) might become invalid at time \(T+1\) because our knowledge has evolved.
		Models would have to be retrained to incorporate up-to-date knowledge, but constantly retraining models can be economically and environmentally costly.
		Continual learning might provide opportunities to develop AI systems that can continually acquire and update their knowledge\cite{wang2024comprehensivesurveycontinuallearning}.

	\subsection{Omics data challenges}
		processing, batch effect
		availability missingness
		weakly paired data
		consider longitudinal studies
		evolution towards single cell
		labeling issue missing labels(semisupervised) low quality labels
		subtype redefine labels ?

	\subsection{single-cell}
		applied on bulk-experiments

	\subsection{Foundation models}

		https://www.science.org/doi/10.1126/science.abc4552

		Lung Cancer Subtype Diagnosis using Weakly-paired Multi-omics Data~\cite{Wang2022}
		subtype; miRNA, SNV, DNAm, WSI
		IF
		encode each omics individually with an attention based encoder
		how are obtained the attention weight ??
		from latent representation, predict the subtype (first loss component)
		how they handle missing omics: GAN based on the combination of the available modalities

		missingness
		A Variational Information Bottleneck Approach to Multi-Omics Data Integration
		\cite{Lee2021AVI}
		mRNA, DNAm, miRNA, Protein
		learning from incomplete multi-view observations
		omics = views handle missingness patterns
		IF, view specific encoders = obtain a latent representation
		product of experts get a joint representation
		predictor from the joint repr and one predictor for each encoder

\end{document}
