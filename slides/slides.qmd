---
title: "Developping and exploring the interest of deep learning approaches in the field of multi-omics data"
subtitle: "Développer et explorer l'intérêt des approches de deep learning dans le domaine des données multi-omiques"
author: "Aurélien Beaude"
date: 06/12/2024
format:
  revealjs: 
    progress: true
    theme: [simple, title-slide.scss]
    slide-number: true
    show-slide-number: all
    margin: 0.025
    width: 1200
    include-in-header:
      - file: mathjax-color.html
    template-partials:
      - title-slide.html
    revealjs-plugins:
      - pointer
      - animate
    filters: 
      - animate
---

## Towards a personalized medicine

```yaml { .animate src="IntroductionPersoMedicine-MédecineTrad_Perso.svg"}
setup:
  - element: "#Title1"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "0"
  - element: "#Patient1"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "1"
  - element: "#Diagnosis1"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "2"
  - element: "#Prognosis1"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "3"
  - element: "#Treatment1"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "3"
  - element: "#TreatmentOK"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "4"
  - element: "#TreatmentKO"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "5"
  - element: "#Title2"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "6"
  - element: "#Patient2"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "6"
  - element: "#Diagnosis2"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "7"
  - element: "#Prognosis2"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "8"
  - element: "#Treatment2"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "9"
```

## Omics data 

![](Omics.svg){fig-align="center"}

## Precision medicine

![](IntroductionPersoMedicine-SlideIntro.svg){fig-align="center" .r-stretch width="110%"}


## Outline 

::: {layout-ncol="3"}

:::: {#first-column}
### Single omics

::: {style="font-size: 70%; padding-left: 0.7em;"}
  - State of the Art
  - AttOmics
  - Results

:::

:::

::: {#second-column}
### Multi Omics

::: {style="font-size: 70%; padding-left: 0.7em;"}
  - State of the Art
  - CrossAttOmics
  - Results
  - CrossAttOmicsGate

:::

:::

::: {#third-column}
### Interpretability

::: {style="font-size: 70%; padding-left: 0.7em;"}
  - Counterfactuals
  - GAN
  - Results

:::

:::

::::

### Conclusions and Perspectives

# Single omics

## State of the art 

![](SOTA_single_omics.svg){fig-align="center"}

## Limits {.smaller}

### Classical deep learning limits
::: {style="font-size: 90%;"}
  + High number of parameters
  + With limited number of examples
  + Risk of overfitting

:::

:::{.callout-important appearance="simple"}
**How to reduce the number of parameters ?**
:::
### Gene expression impacts patient differently
::: {style="font-size: 90%;"}
+ With classical DL, features interactions are learned during training and fixed for inference
:::
  
:::{.callout-important appearance="simple"}
**How to compute interaction specific to each patient ?**
:::

## (Self)-Attention mechanism

:::: {.columns}

::: {.column width="75%"}

:::{.callout-note appearance="simple"}
How to pick relevant information from input data $X = [x_i]_{1 \leq i \leq L}$ ?
:::
::: {style="font-size: 65%;"}
* **Self-attention** allows to capture the associated context of each input element by interacting with other elements
$$
\operatorname{Attention}\left({\color{query}Q},{\color{key}K},{\color{value}V}\right) = \operatorname{softmax}\left(\frac{{\color{query}Q}{\color{key}K^T}}{\sqrt{d_k}}\right){\color{value}V}
$$
$\color{query}Q = \left[q_i\right]_{1 \leq i \leq L}$ with $\color{query}q_i = X_i \cdot W_q$, $\color{key}K$ and $\color{value}V$ are obtained similarly.
 * Quadratic complexity (Space and Time)
:::

::: {style="font-size: 70%"}
:::{.callout-important appearance="simple"}
**How to apply self-attention to large large vectors ?**
:::

:::{.callout-tip appearance="simple"}
Group related features together and apply self-attention on it
:::
:::

:::

::: {.column width="25%"}
![](Attention.svg){fig-align="right" width=90%}
:::

::::

## AttOmics Architecture

:::: {.columns}

::: {.column width="60%" }

![](AttOmics-ArchitectureFull.svg){fig-align="left" width=90%}

:::

::: {.column width="40%"}

:::{.r-stack}

::: {.fragment .fade-in-then-out fragment-index=1}

::: {style="font-size: 70%;"}
**Grouping Strategies**

* Random
* Clustering
* Knowledge:
  * Gene ontology
  * Cancer Hallmarks

$$
\begin{align}
X_G &= \mathcal{T}\left(X\right) \\
 &= \left[X_{g_1}, \cdots, X_{g_4}\right]
\end{align}
$$

:::

:::

::: {.fragment .fade-out fragment-index=5}

::: {style="font-size: 70%;"}

::: {.fragment .fade-in fragment-index=2}
**Intra-group interactions**
:::

::: {.fragment .fade-in fragment-index=3}

:::{.callout-important appearance="simple"}
**Gene grouping creates unwanted restrictions**

Genes in group $g_1$ cannot interact with genes from other groups
:::

:::

::: {.fragment .fade-in fragment-index=4}

:::{.callout-tip appearance="simple"}
Restore group interactions with the Attention mechanism
:::

:::

::: {.fragment .fade-in fragment-index=2}

$$ 
X'_{g_i} = \operatorname{FCN}\left(X_{g_i}\right)
$$

$$ 
X'_G = \left[X'_{g_1}, \cdots, X'_{g_4}\right]
$$
:::

:::

:::

::: {.fragment .fade-in fragment-index=5}

::: {style="font-size: 70%;"}
**Inter-groups interactions** 

:::{.callout-note appearance="simple"}
New representation of each group taking into consideration features from other groups
:::

$$
\begin{align}
\color{query}q_i &= \color{query}X'_{g_i} \cdot W_q^h \\
\color{key}k_i &= \color{key}X'_{g_i} \cdot W_k^h \\
\color{value}v_i &=\color{value} X'_{g_i} \cdot W_v^h 
\end{align}
$$

$$
\begin{align}
Z &= \operatorname{MultiHeadAttention}\left(X'_G\right) \\
 &= \operatorname{concat}\left(\left[h_1, \cdots, h_H \right]\right) \\
 h_i &= \operatorname{Attention}\left({\color{query}Q},{\color{key}K},{\color{value}V}\right)
\end{align}
$$

:::

:::

:::

:::

:::

:::

:::

::::


## Results

![](AttOmicsLimitedTraining.svg){.r-stretch fig-align="center"}

## Cancer Signature {.smaller}

:::: {.columns}

::: {.column width="70%" }
![](AttOmicsCancerSignatures.png)
:::

::: {.column width="30%" }

* Cancer heatmaps are a mean of patient heatmaps with the same cancer
* Each square of the heatmap represents the attention weight between two different group learnt by the model

::: {.fragment .fade-up fragment-index=1}
::: {.fragment .highlight-red	fragment-index=1}
**Across cancers different interactions are learnt**
:::
:::

:::

::::

## CESC cancer {.smaller}

:::: {.columns}

::: {.column width="60%" }
![](AttOmicsCESCPathways.svg){width=90%}
:::

::: {.column width="40%" }
**Identified pathways:**

* IL6 JAK STAT3 signaling
* Hedgehog signaling
* WNT singaling

**Identified interactions:**

* WNT and Hedgehog cross-talk involved in chemo-resistant cervical cancer
:::
::::

## AttOmics Conclusions

* Proposed a grouping mechanism to scale the self-attention to omics data
* AttOmics has less parameters and achieves similar or better performances
* AttOmics has better performances with a limited training set
* Self-attention: capture patient specific feature interactions

:::{style="font-size=90%;"}
:::{.callout-important appearance="simple"}
Omics were analyzed individually but a phenotype results from their interaction
:::

:::{.callout-tip appearance="simple"}
Combine the different omics in a single model.
:::
:::

# Multi omics

## Multimodal AI

![](SOTA_MultiOmics.svg)

## Attention as an integration strategy {.smaller}

:::{.callout-note appearance="simple"}
Attention mechansim can capture interaction between two vectors
:::

:::: {.columns}

::: {.column width="40%" }
### Early Fusion

![](AttentionEF.svg){fig-align="center" width=48%}

:::{.callout-important appearance="simple"}
* High dimensionnality
* Attention complexity: $\mathcal{O}(n^2)$
:::

:::

::: {.column width="60%"}
### Intermediate Fusion

:::: {.columns}

::: {.column  #vcenter width="45%"}

::: {layout="[[-1], [-1], [1], [-1]]" style="font-size: 80%;"}

\begin{align}
Z_{\beta \rightarrow \alpha} &=  \operatorname{CrossAtt}_{\beta \rightarrow \alpha}\left(X_{\alpha}, X_{\beta} \right) \\
&= \operatorname{Attention}\left(Q_{\alpha},K_{\beta},V_{\beta} \right)
\end{align}


:::
::: 

::: {.column width="50%" }
![](AttentionIF.svg){fig-align="center" width=80%}
:::
::::

:::{.callout-important appearance="simple"}
Consider all modality pairs: $n(n-1)$ pairs to consider
:::

:::{.callout-tip appearance="simple"}
Only consider pairs known to interact
:::

:::

::::

## CrossAttOmics

![](CrossAttOmics.svg)

## Omics combination

![](tcga_perf_omics_comb.svg){fig-align="center" .lightbox .r-stretch}

::: {.fragment .fade-up}

:::

## Identifying important interactions

:::: {.columns}

::: {.column width=40%}

**LRP** 

```yaml { .animate src="LRP.svg"}
setup:
  - element: "#LRP"
    modifier: attr
    parameters:
      - class: fragment
        data-fragment-index: "0"
```

![](CrossAttOmicsDetailsLRP.svg){width=90%}
:::

::: {.column width=60%}
![](CrossAttOmicsLRP.svg){width=100%}
:::
::::


## Robustness to missing modalities

:::: {.columns}

::: {.column width="33%"}


::: {layout="[[1], [1, 1]]"}
![](missing_modality_pattern.svg)

![](missing_modality_latent.svg)

![](missing_modality_reconstruction.svg)
:::

:::

::: {.column width="67%"}
![](CrossAttOmicsModalityDropout.svg){fig-align="center" width=90%}
:::

::::

## CrossAttOmicsGate: a data-driven approach

![](CrossAttOmicsGate.svg){.r-stretch}


## CrossAttOmicsGate results {.smaller}

|  Model  | Accuracy      | Precision     | Recall          | F1            |
|---------|--------------:|--------------:|----------------:|--------------:|
| No Gate | 0.980 ± 0.001 | 0.982 ± 0.002 | 0.979 ± 0.002   | 0.980 ± 0.002 |
| Gate    | 0.987 ± 0.001 | 0.989 ± 0.001 | 0.987 ± 0.001   | 0.987 ± 0.001 |

![](CrossAttOmicsGateAlphaHeatmap.svg){fig-align="center"}

## Multi-omics conclusions 

:::{.incremental}
* Harness cross-attention to build a multimodal representation that explicitly consider interactions between modalities
* Using only two or three non-protein omics combination, achieve similar accuracy to what is obtained by training only on proteins
* Modality dropout is an effective strategy to increase robustness to missing modalities
* Data-driven strategy to identify important interactions
:::

:::{.callout-tip appearance="simple"}
Patient can be diagnosed efficiently but what are the disease drivers ? How to treat the patient ?
:::

# Interpretability

## Counterfactuals properties


![](counterfactualsProperties.svg){fig-align="center" .r-stretch}

::: {.incremental}
* Data manifold closeness = respect the original data distribution
* GANs captures the data distribution (Goodfellow et al., 2014)
:::

## Generating counterfactuals with a GAN

![](ConterfactualGAN.svg){.r-stretch}

<!-- * $\mathcal{L}_{\text{WGAN}} =$ -->
<!-- * $\mathcal{L}_{\operatorname{CL}} = \operatorname{CE}\left(\operatorname{CL}\left(x^{\text{CF}}\right), y^{\text{CF}}\right)$ -->

## Adversarial training of $\operatorname{Cl}$

::: {.incremental}
* An **adversarial example**, $x^{\star} = x + \delta$, is an instance $x$ with the addition of a small perturbation
$\delta$ that is incorrectly predicted by a model
* They are obtained by solving $\max_{\delta \in \Delta} \mathcal{L}\left(\operatorname{Cl}\left(x + \delta\right), y\right)$ with gradient ascent. (ex: projected gradient ascent)
* Adversarial training:
$$
\min_{\operatorname{Cl}} \mathbb{E}_{(x,y)}\left[\max_{\delta^{\star} \in \Delta} \mathcal{L}_{\text{CE}}\left(\operatorname{Cl}\left(x+\delta^{\star}\right), y \right)  \right]
$$
:::

## Results

::::{.columns}

:::{.column width=70% }
:::{style="font-size:80%;"}
* Counterfactual Accuracy ($y^{\text{CF}}$ ✅︎ and $y_{\text{Tissue}}$ ✅︎): $\mathcal{A}_{\text{CF}} = 0.96$
* Distance to the original point: $L_1$, $L_2$, $L_{\infty}$
* Sparsity: $L_0 = \sum_i \left|x_i \right|^{0}$ with $\left|0\right|^0 = 0$
* kNN Accuracy: $\mathcal{A}_{\text{kNN}} = \frac{1}{k} = \sum_{i=1}^{k} \mathbb{1}\left(y^{\text{CF}}, y_{i}^{\text{kNN}}\right)$
:::

![](kNN_Accuracy.svg){fig-align="center" width=50%}
:::

:::{.column width=30%}
:::{style="font-size:70%;"}
|                               |      |
|-------------------------------|------|
| $L_1$                         | 2440 |
| $L_2$                         | 30   |
| $L_{\infty}$                  | 1    |
| $\mathcal{A}_{\text{kNN}}$    | 0    |
| $\mathcal{A}_{\text{Oracle}}$ | 0.94 |
:::
![](InterClassDistances.svg){fig-align="center"}
:::

::::

## Results

![](BRCA_CF_UMAP.svg){fig-align="center"}

## Results

![](BRCA_CF.svg){.r-strech fig-align="center"}

:::{style="font-size:40%;"}
GDA: gene-disease association from DisGenet / COSMIC: Catalogue of somatic mutations in cancer
:::

# Conclusions and Perspectives

## Conclusions

:::{.incremental style="font-size:90%;"}
* **AttOmics**: Applied self-attention mechanism to omics profile tp capture patient-specific interactions. Self-attention was applied to groups of features which allowed the addition of knowledge in the groups.
* **CrossAttOmics**: Integrate multi-omics data based on the known regulatory interactions between modalities and the cross-attention. 
* **CrossAttOmicsGate**: Let the network score each interaction with a gating mechanism
* **Counterfactual Generation**: Find the perturbation on the molecular profile that will change the prediction from a disease state to a health
:::

## Perspectives: Scalar Attention

:::: {.columns}

:::{.column width=65%}

* In AttOmics, groups are an architectural constraints
* Removing groups: directly consider feature interactions

:::{.callout-important appearance="simple"}
How to compute attention between scalar values ? 
:::


$$
A_{ij} = \operatorname{softmin}\left(\left|Q_{i} - K_{j} \right| \right)
$$


:::{.callout-tip appearance="simple"}
Efficient implementation with Triton
:::
:::

:::{.column width=35%}
![](ScalarAttentionMemory.svg){fig-align="right" width=100%}
:::

::::

## Perspectives: Knowledge

* Biolgically-informed architectures are common when applying deep learning to biological problems as they provide a strong inductive bias.

:::{.callout-important appearance="simple"}
Knowledge is incomplete or may contains errors. How to handle this ?
:::

:::{.callout-important appearance="simple"}
What to do about unnannotated features ? 
:::

## {.center}
::: {.r-fit-text}
Thank you
:::

# Appendix
